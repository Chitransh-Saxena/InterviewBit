-> Time complexity is not the time taken to run some algorithm on some machine.
-> Consider we have 2 machines, 1 is fast and 1 is slow. Both are given some algorithm to run. Faster one runs it in 5 secs and slower one does in 10 seconds for a certain amount of data.
    -> Now, in this case which device has a better time complexity?
    -> Neither one, because time complexity is not a concept of the hardware.
    -> Time complexity is basically a relationship between time taken to run an algorithm vs the amount of input provided.

-> So, how do we understand time complexity?
    -> If we start understanding how does the time taken vary, based on the amount of input, we can propose a relationship between the two items.
    -> Example: If I pick linear search, I want to find something in a dataset, and consider it does not exist in the dataset.
    -> So, how would the graph look? Check image time_complexity_analysis in this folder.
    -> Point being, no matter which machine we run the algorithm in, the time may reduce, but the relationship between time taken vs the size of datas remains the same.
    -> In case of linear search, it was forming a straight line, which we can represent in the form of a linear equation (y = mx + c)